{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import VOCDetection\n",
    "from utils.transform import train_transform\n",
    "from utils.transform import val_transform\n",
    "\n",
    "train_data = VOCDetection(root='./data', year='2012', image_set='train',\n",
    "                             download=True, transforms=train_transform)\n",
    "\n",
    "val_data = VOCDetection(root='./data', year='2012', image_set='train',\n",
    "                             download=True, transforms=val_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "print('The ImageNet train set is ready. Size : {}'.format(len(train_loader)*batch_size))\n",
    "\n",
    "\n",
    "dataloaders = dict()\n",
    "\n",
    "dataloaders['train'] = train_loader\n",
    "dataloaders['val'] = val_loader\n",
    "\n",
    "dataset_sizes = {'train': len(train_loader)*batch_size,\n",
    "                'val': len(val_loader)*batch_size}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import visualize_bb\n",
    "from utils.dataset import inverse_target\n",
    "\n",
    "samples  =[]\n",
    "\n",
    "for i in range(15,22):\n",
    "  img , targets = train_data[i]\n",
    "  targets = [target.unsqueeze(0) for target in targets]\n",
    "  bboxes, labels = inverse_target(targets)\n",
    "  sample = {'image': img, 'bbox':bboxes[1], 'labels':labels[1]}     #should plot properly for all S[i]\n",
    "  samples+=[sample]\n",
    "\n",
    "\n",
    "visualize_bb(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation and Loss function Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.loss import YoloV4_Loss\n",
    "from model.yolov4 import YoloV4\n",
    "\n",
    "model = YoloV4(num_classes = 20)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "#test loss on random inputs\n",
    "g = torch.Generator().manual_seed(0)\n",
    "\n",
    "preds  = [torch.randn((32, s,s,3,25),generator=g) for s in S]\n",
    "ground_truths =[torch.randn((32, s,s,3,25),generator=g) for s in S]\n",
    "\n",
    "for ground_truth in ground_truths:\n",
    "  ground_truth[..., 0] = torch.empty_like(ground_truth[..., 0], dtype=torch.long).random_(2)\n",
    "  ground_truth[...,5:] = torch.empty_like(ground_truth[..., 5:], dtype=torch.long).random_(2)\n",
    "\n",
    "loss = YoloV4_Loss()\n",
    "print('Loss1:', loss(preds, ground_truths))\n",
    "\n",
    "\n",
    "#test loss on model \n",
    "img, targets = train_data[0]\n",
    "img = img.to(device)\n",
    "targets = [target.unsqueeze(dim=0).to(device) for target in targets]\n",
    "pred = model(img.unsqueeze_(dim=0))\n",
    "print('Loss2:' ,loss(pred, targets))\n",
    "\n",
    "\n",
    "#Model output verification \n",
    "x = model(torch.rand(5, 3, 416, 416).to(device))\n",
    "print(\"output shapes:\", x[0].shape, x[1].shape, x[2].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training \\n')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = YoloV4(num_classes = 20)\n",
    "model = model.to(device)\n",
    "\n",
    "#loss function \n",
    "criterion = YoloV4_Loss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.train import train_model\n",
    "\n",
    "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, \n",
    "                    dataloaders, dataset_sizes, num_epochs=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from utils.utils import visualize_outputs\n",
    "\n",
    "indices = np.random.randint(0, 500, size=5)\n",
    "visualize_outputs(indices, model, val_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
